###################################################################
# stream2segment config file to tune the functionality.
# IMPORTANT: DO NOT EDIT THIS FILE. COPY THIS FILE AS config.yaml IN THE SAME DIRECTORY AND EDIT THE LATTER
###################################################################

###################
# POGRAM PARAMETERS
###################

# Database url where to save data. Currently supported are sqlite and postgresql databases.
# If sqlite database, just write the path to your local file
# prefixed with 'sqlite:///' (e.g., 'sqlite:////home/my_folder/db.sqlite'): non-absolute
# paths will be considered relative to the config file they are written in.
# If not sqlite, the syntax is:
# dialect+driver://username:password@host:port/database
# (e.g.: 'postgresql://smith:Hw_6,9@hh21.uni-northpole.org/stream2segment_db')
# (for info see: http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls)
dburl: 'sqlite:////path/to/my/db.sqlite'

# Try to dowonload again already saved segments whose response does not have a status code.
# This indicates a failed download for any reason not included in the others listed here (e.g.,
# when queries for several channels like 'HH?' are made and not all expected channels are returned)
retry_no_code: true
# Try to dowonload again already saved segments whose response could not be retrieved because of a
# general url error (e.g., no internet connection)
retry_url_errors: true
# Try to dowonload again already saved segments whose response was successful but with corrupted / malformed data
retry_mseed_errors: false
# Try to dowonload again already saved segments whose response returned a status code in the
# 400-499 range (Client errors)
retry_4xx: false
# Try to dowonload again already saved segments whose response returned a status code in the
# 500-599 range (Server errors)
retry_5xx: true

# Set if station inventories should be downloaded. 
# If true, the inventory .xml file will be saved for each station which has not its inventory
# already saved and has at least one downloaded segment with non-empty data.
# In case of download errors, nothing is saved.
# Inventories  can be used for removing the intrumental response of the segments, set to True if
# you later need this feature for processing. Inventories are saved compressed because
# they are relatively "big" in size. Moreover, their download slows down the segments
# download, and you can always fetch and save it to the processing
# Optional: defaults to false if missing
inventory: false


#######################
# DATA QUERY PARAMETERS
#######################

# Limit to events (and datacenters) on or after the specified start time.
# Optional: missing value defaults to yesterday at midnight
start: 2006-01-01T00:00:00
# Limit to events (and datacenters) on or before the specified end time.
# Optional: missing value defaults to today at midnight
end: 2016-12-25T00:00:00

# the event web service (url)
eventws: 'http://seismicportal.eu/fdsnws/event/1/query'

# a dict of fdns ws arguments for the eventws query. All values are permitted except 'format', 'start' 
# and 'end' (the latter are taken from the values of the relative config parameters specified above)
eventws_query_args:
  minmag: 3.0
  minlat: 47.0
  maxlat: 57.0
  minlon: 4.0
  maxlon: 17.0
  mindepth: 1
  maxdepth: 50

# data-select service to use. Currently implemented are "eida" and "iris". Missing values will
# get all datacenters from the given routing service (see 'routing_service_url' in advanced_settings)
service: 'eida'

# stations will be downloaded with level='channel'. Set here which channels to download. For info see e.g.:
# https://ds.iris.edu/ds/nodes/dmc/tools/data_channels/#???
channels:
 - "HH?"
 # - 'SH?'
 - 'HN?'
 # - 'SN?'
 - 'HL?'
 # - 'SL?'


# search radius: for each event, stations will be searched within a circular area whose radius is a linear function
# of the event magnitude:
#
#                   |
#     maxmag_radius +                oooooooooooo
#                   |              o
#                   |            o
#                   |          o
#     minmag_radius + oooooooo
#                   |
#                   ---------+-------+------------
#                         minmag   maxmag
# Edge cases:
# - if minmag_radius == maxmag_radius = R, this is equivalent to a constant function returning always R regardless of the magnitude
# - if minmag_radius != maxmag_radius and minmag == maxmag = M, this function returns minmag_radius for all magnitudes < M,
#     maxmag_radius for all magnitudes > M, and (minmag_radius + maxmag_radius) / 2 for all magnitudes == M
search_radius:
 minmag: 6 # min magnitude
 maxmag: 7 # max magnitude
 minmag_radius: 3 # search radius for min mag (deg)
 maxmag_radius: 3 # search radius for max mag (deg)
 
# Limit the search to station channels with at least min_sample_rate (in Hz).
# The relative segments are *mot likely* (not always) matching the channel sample rate.
# Set to 0 or negative number to ignore the sampling rate
min_sample_rate: 60

# Set here the phases to be calculated to asses the travel times of a wave from
# the event location to each station location. The minimum of all travel times will be set as the
# arrival time AT of a specific event at a specific station, and waveform data will be queried to
# the station's datacenter in time a window around AT (see parameter wtimespan below). The more items
# you set here below, the more likely a minimum is found, but the more time is needed to calculate
# all of them.
# NOTE:
# This parameter is ignored for already downloaded segments, so if you try to adjust the
# arrival time for all downloaded segments you must run a new download from scratch (e.g., specifying a new
# databse output with dburl)
# For info see: https://docs.obspy.org/packages/obspy.taup.html#phase-naming-in-obspy-taup
traveltime_phases: 
 - 'P'
 - 'p'
 - 'Pn'

# Waveform segment time window: specify two positive integers denoting the 
# minutes to account for before and after the calculated arrival time
wtimespan:
 - 1.0 # start time of the waveform segment to download, in minutes *before* the previously calculated arrival time
 - 3.0 # end time of the waveform segment to download, in minutes *after* the previously calculated arrival time


######################################################################################################
# Advanced settings (in principle, you should not care about these unless you know what you are doing)
######################################################################################################

advanced_settings:
 # the routing service used to fetch the datacenter(s) and relative network/stations
 routing_service_url: "http://rz-vm258.gfz-potsdam.de/eidaws/routing/1/query"
 # size of each block of data requested when downloading data (in bytes) until no data is available.
 # If 0 or negative, all data will be read in a single call (if 0, it will be converted to -1).
 download_blocksize: 1048576  # = 1024*1024
 # how many parallel threads to start when downloading (one thread per download)
 # If 0 or negative, is automatically set according to the machine CPU
 # (https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor)
 max_thread_workers: 0
 # max time to wait (in seconds) while downloading stations (`urllib2.urlopen` timeout argument)
 s_timeout: 120
 # If the flag to download station inventories is on (true), max time to wait (in seconds) while downloading
 #station inventories (`urllib2.urlopen` timeout argument)
 i_timeout: 60
 # max time to wait (in seconds) while downloading waveform data (`urllib2.urlopen` timeout argument)
 w_timeout: 60
 # the buffer size used when writing items (stations, segments, events, ...) to database.
 # Increasing this number to speed up db IO operations. Keep in mind that if any item in the buffer
 # cannot be inserted or updated (e.g., integrity errors), all subsequent buffer items will also be discarded.
 # If you want to be sure that only items that cannot be inserted/updated are discarded,
 # set it to 1, although for massive downloads it might cost hours of download more
 db_buf_size: 20


