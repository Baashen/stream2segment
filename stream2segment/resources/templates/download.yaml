###################################################################
# stream2segment download config file to tune the functionality.
###################################################################

###################
# POGRAM PARAMETERS
###################

# Database url where to save data. Currently supported are sqlite and postgresql databases.
# If sqlite database, just write the path to your local file
# prefixed with 'sqlite:///' (e.g., 'sqlite:////home/my_folder/db.sqlite'): non-absolute
# paths will be considered relative to the config file they are written in.
# If not sqlite, the syntax is:
# dialect+driver://username:password@host:port/database
# (e.g.: 'postgresql://smith:Hw_6,9@hh21.unibo.org/s2s_db')
# (for info see: http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls)
dburl: 'sqlite:////path/to/my/db.sqlite'

# Try to dowonload again already saved segments without a download status code (NULL).
# This indicates a failed download for any reason not included in the others listed here (e.g.,
# when queries for several channels like 'HH?' are made, the Response status code is '200: Ok' but
# not all expected channels segments are returned)
retry_no_code: true
# Try to dowonload again already saved segments whose response could not be retrieved because of a
# general url error (e.g., no internet connection)
retry_url_errors: true
# Try to dowonload again already saved segments whose response was successful but with corrupted / malformed data
retry_mseed_errors: false
# Try to dowonload again already saved segments whose response returned a status code in the
# 400-499 range (Client errors)
retry_4xx: false
# Try to dowonload again already saved segments whose response returned a status code in the
# 500-599 range (Server errors)
retry_5xx: true
# Try to download again already saved segments whose response was Ok but returned all chunks of data (records)
# outside the requested time span (see 'wtimespan' for details). Segments of this type have no data
# saved (empty)
retry_timespan_errors: true
# Try to download again already saved segments whose response was Ok but returned some chunks of data (records)
# outside the requested time span (see 'wtimespan' for details). Segments of this type have at least one byte
# of data saved (the records inside the request timespan)
retry_timespan_warnings: false

# Set if station inventories should be downloaded. 
# If true, inventory data (xml format) will be saved for each station whithout inventory data
# and at least one downloaded segment with downloaded data.
# You can always fetch and save inventories later during processing, if required
inventory: true


#######################
# DATA QUERY PARAMETERS
#######################

# Limit to events (and datacenters) on or after the specified start time. Specify a date or date-time in iso-format
# or a non-negative integer to denote the number of days before today at midnight.
# Example: start=1 and end=0 => fetch events occurred yesterday
start: 2006-01-01T00:00:00
# Limit to events (and datacenters) on or before the specified end time. Specify a date or date-time in iso-format
# or a non-negative integer to denote the number of days before today at midnight.
# Example: start=1 and end=0 => fetch events occurred yesterday
end: 2016-12-25T00:00:00

# the event web service to use (url)
eventws: 'http://seismicportal.eu/fdsnws/event/1/query'

# a dict of fdns ws arguments for the eventws query. All values are permitted except 'format', 'start' 
# and 'end' (the latter are taken from the values of the relative config parameters specified above)
eventws_query_args:
  minmag: 3.0
  minlat: 47.0
  maxlat: 57.0
  minlon: 4.0
  maxlon: 17.0
  mindepth: 1
  maxdepth: 50

# data-select web service to use (url). It should be fdsn-ws compliant, so that the relative
# station url can be retrieved automatically. You can also type two special values, "iris" and
# "eida". The former is just a shortcut for the iris data-select url, the latter will automatically
# fetch all eida datacenters and their urls
dataws: 'eida'

# channels to be downloaded for each station found
channels:
 - "HH?"
 # - 'SH?'
 - 'HN?'
 # - 'SN?'
 - 'HL?'
 # - 'SL?'

# search radius: for each event, stations will be searched within a circular area whose radius is a linear function
# of the event magnitude:
#
#                   |
#     maxmag_radius +                oooooooooooo
#                   |              o
#                   |            o
#                   |          o
#     minmag_radius + oooooooo
#                   |
#                   ---------+-------+------------
#                         minmag   maxmag
# Edge cases:
# - if minmag_radius == maxmag_radius = R, this is equivalent to a constant function returning always R regardless of the magnitude
# - if minmag_radius != maxmag_radius and minmag == maxmag = M, this function returns minmag_radius for all magnitudes < M,
#     maxmag_radius for all magnitudes > M, and (minmag_radius + maxmag_radius) / 2 for all magnitudes == M
search_radius:
 minmag: 6 # min magnitude
 maxmag: 7 # max magnitude
 minmag_radius: 3 # search radius for min mag (deg)
 maxmag_radius: 3 # search radius for max mag (deg)
 
# Limit the search to station channels with at least min_sample_rate (in Hz).
# The relative segments are *mot likely* (not always) matching the channel sample rate.
# Set to 0 or negative number to ignore the sampling rate
min_sample_rate: 60

# The model to be used to asses the travel times of a wave from
# the event location to each station location. For each segment, the arrival time (travel
# time + event time) will be the pivot whereby the user sets up the download time window
# (see also 'wtimespan').
# A model is internally a 2- or 3-dimensional grid
# of pre-computed travel time points coupled with an highly efficient
# algoritm which returns interpolated travel time values. From experiments comparing the algorithm
# and the "real" function values, we observed max errors < 0.5 seconds and a relevant increase in
# execution speed.
# The available default models, all assuming receiver depth=0 for simplicity, are:
# ak135_ttp+: ak135 model pre-computed for all ttp+ phases (P wave arrivals)
# ak135_tts+: ak135 model pre-computed for all tts+ phases (S wave arrivals)
# iasp91_ttp+: iasp91 model pre-computed for all ttp+ phases (P wave arrivals)
# iasp91_tts+: iasp91 model pre-computed for all tts+ phases (S wave arrivals)
traveltimes_model: 'ak135_ttp+'

# The segments time window to download: specify two positive floats denoting the 
# minutes to account for before and after the calculated arrival time.
# Note that 3.5 means 3 minutes 30 seconds, so 3 minutes and 1 second should be 3.01666666, but you
# should never need such a fine resolution. Note that all segment time windows will be rounded
# to the closest second to avoid floating point errors when checking for already existing segments
wtimespan:
 - 1.0 # start time of the waveform segment to download, in minutes *before* the previously calculated arrival time.
 - 3.0 # end time of the waveform segment to download, in minutes *after* the previously calculated arrival time


######################################################################################################
# Advanced settings (in principle, you should not care about these unless you know what you are doing)
######################################################################################################

advanced_settings:
 # the routing service used to fetch the eida nodes and relative network/stations
 routing_service_url: "http://rz-vm258.gfz-potsdam.de/eidaws/routing/1/query"
 # size (in bytes) of each block of data requested when downloading until no data is available.
 # This setting holds for any kind of data downloaded (event, waveform or station metadata)
 # If 0 or negative, all data will be read in a single call (if 0, it will be converted to -1).
 download_blocksize: 1048576  # = 1024*1024
 # how many parallel threads to start when downloading (one thread per download)
 # If 0 or negative, is automatically set according to the machine CPU
 # (https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor)
 max_thread_workers: 0
 # max time to wait (in seconds) for a single request while downloading stations+channel metadata (`urllib2.urlopen` timeout argument)
 s_timeout: 120
 # If the flag to download station inventories is on (true), max time to wait (in seconds) for a single request
 # while downloading an inventory in xml format (`urllib2.urlopen` timeout argument)
 i_timeout: 60
 # max time to wait (in seconds) for a single request while downloading waveform data (`urllib2.urlopen` timeout argument)
 w_timeout: 60
 # the buffer size used when writing items (stations, segments, events, ...) to database.
 # Increasing this number to speed up db IO operations. Keep in mind that if any item in the buffer
 # cannot be inserted or updated (e.g., integrity errors), all subsequent buffer items will also be discarded.
 # If you want to be sure that only items that cannot be inserted/updated are discarded,
 # set it to 1, although for massive downloads it might cost hours of download more
 db_buf_size: 20


