###################################################################
# stream2segment download config file to tune the functionality.
###################################################################

###################
# POGRAM PARAMETERS
###################

# Database url where to save data. Currently supported are sqlite and postgresql databases.
# If sqlite, just write the path to your local file
# prefixed with 'sqlite:///' (e.g., 'sqlite:////home/my_folder/db.sqlite'): non-absolute
# paths will be considered relative to the config file they are written in.
# If not sqlite, the syntax is:
# dialect+driver://username:password@host:port/database
# (e.g.: 'postgresql://smith:Hw_6,@mymachine.example.org/mydb')
# (for info see: http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls)
dburl: 'sqlite:////path/to/my/db.sqlite'

# Try to dowonload again already saved segments with no waveform data because not found
# in the response. This is NOT the case when the server returns no data with an appropriate
# 'No Content' message, but when a successful response ('OK') does not contain
# the expected segment data. For instance, a multi-segment request might return some but not all requested segments.
retry_seg_not_found: true
# Try to dowonload again already saved segments with no waveform data because of a
# general url error (e.g., no internet connection, timeout, ...)
retry_url_err: true
# Try to dowonload again already saved segments with no waveform data because the response was
# not readable as MiniSeed
retry_mseed_err: false
# Try to dowonload again already saved segments with no waveform data because of a client error
# (response code in [400,499])
retry_client_err: false
# Try to dowonload again already saved segments with no waveform data because of a server error
# (response code in [500,599])
retry_server_err: true
# Try to download again already saved segments with no waveform data because the response data
# was completely outside the requested time span (see 'timespan' for details)
retry_timespan_err: true

# Tells to update segments metadata, i.e. overwrite data of already saved segments and stations
# (keeping their id unchanged). New station and new channels (not found on the db) will be saved in any case.
# This parameter might affect how the 'inventory' option beahves (see 'inventory' for details)
update_metadata: false

# Tells to download station inventories (xml format):
# inventories will be downloaded for all station that have saved segments (with data and no errors), and
# if the metadata should not be updated (see 'update_metadata') already saved inventories will not be downloaded again.
# You can always fetch and save inventories later during processing, if required
inventory: true


#######################
# DATA QUERY PARAMETERS
#######################

# Limit to events (and datacenters) on or after the specified start time. Specify a date or date-time in iso-format
# or an integer >=0 to denote the number of days before today at midnight.
# Example: start=1 and end=0 => fetch events occurred yesterday
start: 2006-01-01T00:00:00
# Limit to events (and datacenters) on or before the specified end time. Specify a date or date-time in iso-format
# or an integer >=0 to denote the number of days before today at midnight.
# Example: start=1 and end=0 => fetch events occurred yesterday
end: 2016-12-25T00:00:00

# the event web service to use (url)
eventws: 'http://seismicportal.eu/fdsnws/event/1/query'

# a dict of fdns ws arguments for the eventws query. All values are permitted except 'format', 'start' 
# and 'end' (the latter are taken from the values of the relative config parameters specified above)
eventws_query_args:
  minmag: 3.0
  minlat: 47.0
  maxlat: 57.0
  minlon: 4.0
  maxlon: 17.0
  mindepth: 1
  maxdepth: 50

# data-select web service to use (url). It should be fdsn compliant, so that the relative
# station url can be retrieved automatically. You can also type two special values, "iris" and
# "eida". The former is just a shortcut for the iris data-select urls
# (https://service.iris.edu/fdsnws/station/1/query for stations and channels,
# https://service.iris.edu/fdsnws/dataselect/1/query for data), the latter will automatically
# fetch all eida datacenters and their urls.
# For any datacenter which fails to download stations and channels (e.g., network error, client/server error)
# the database will be queried with the same filters (see network, station, location, channel and min_sample_rate, if provided)
# after converting FDSN syntax to SQL, when needed.
# If no datacenter station's url returns data and the db is empty (first download) the download process will stop
dataws: 'eida'

# Limit the search to stations and channels matching any of the four parameters:
# network, station, location, channel. E.g.:
# netowrk: '!A*'  (download networks NOT starting with 'A')
# channels: 'H*,B??' (download all channels starting with H, OR having 3 characters whose first is 'B')
# If a parameter name is missing, it's value defaults to '*'.
# Parameter values are strings following FDSN queries syntax (https://www.fdsn.org/webservices/FDSN-WS-Specifications-1.1.pdf),
# which is enhanced in here with the operator ! to indicate logical NOT. So e.g.:
# Parameter names can be given also in FDSN standard (e.g. 'net' or 'network' instead of 'networks')
# From the config yaml file you can also specify a list/array of strings in yaml format
# instead of comma-separated strings. E.g., these are quivalent:
# networks: [ "A,B" , "C" ]
# networks: "A,B,C"
# networks:
#  - "A, B"
#  - "C"
channels:
 - "HH?"
 - 'HN?'
 - 'HL?'

# Limit the search to station's channels with at least min_sample_rate (in Hz).
# The relative segments are *mot likely* (not always) matching the channel sample rate.
# Set to 0 or negative number to ignore the sampling rate
min_sample_rate: 60

# search radius: for each event, stations will be searched within a circular area whose radius is a linear function
# of the event magnitude:
#
#                   |
#     maxmag_radius +                oooooooooooo
#                   |              o
#                   |            o
#                   |          o
#     minmag_radius + oooooooo
#                   |
#                   ---------+-------+------------
#                         minmag   maxmag
# Edge cases:
# - if minmag_radius == maxmag_radius = R, this is equivalent to a constant function returning always R regardless of the magnitude
# - if minmag_radius != maxmag_radius and minmag == maxmag = M, this function returns minmag_radius for all magnitudes < M,
#     maxmag_radius for all magnitudes > M, and (minmag_radius + maxmag_radius) / 2 for all magnitudes == M
search_radius:
 minmag: 6 # min magnitude
 maxmag: 7 # max magnitude
 minmag_radius: 3 # search radius for min mag (deg)
 maxmag_radius: 3 # search radius for max mag (deg)

# The model to be used to asses the travel times of a wave from
# the event location to each station location. Type a string denoting a file name (absolute path)
# of a custom model created by means of `s2s utils ttcreate` or one of the 4 built-in models
# (all assuming receiver depth=0 for simplicity):
# ak135_ttp+: ak135 model pre-computed for all ttp+ phases (P wave arrivals)
# ak135_tts+: ak135 model pre-computed for all tts+ phases (S wave arrivals)
# iasp91_ttp+: iasp91 model pre-computed for all ttp+ phases (P wave arrivals)
# iasp91_tts+: iasp91 model pre-computed for all tts+ phases (S wave arrivals)
# For each segment, the arrival time (travel
# time + event time) will be the pivot whereby the user sets up the download time window
# (see also 'timespan').
# A model is internally a 2- or 3-dimensional grid
# of pre-computed travel time points coupled with an highly efficient
# algoritm which returns interpolated travel time values. This results in a relevant increase in
# execution speed, with max errors in the order of 0.5 seconds due to interpolation.
traveltimes_model: 'ak135_ttp+'

# The segment's data time window to download: specify two positive floats denoting the 
# minutes to account for before and after the calculated arrival time.
# Note that 3.5 means 3 minutes 30 seconds, 3.1 means 3 minutes and 6 seconds, and so on:
# whereas this is not straightforward, there should be never the need to specify resolutions finer
# than half a minute; in any case all segment time windows will be rounded
# to the closest second to avoid floating point errors when checking for already existing segments
timespan:
 - 1.0 # start time of the waveform segment to download, in minutes *before* the previously calculated arrival time.
 - 3.0 # end time of the waveform segment to download, in minutes *after* the previously calculated arrival time


######################################################################################################
# Advanced settings (in principle, you should not care about these unless you know what you are doing)
######################################################################################################

advanced_settings:
 # the routing service used to fetch the eida nodes and relative network/stations
 routing_service_url: "http://rz-vm258.gfz-potsdam.de/eidaws/routing/1/query"
 # size (in bytes) of each block of data requested when downloading until no data is available.
 # This setting holds for any kind of data downloaded (event, waveform or station metadata)
 # If 0 or negative, all data will be read in a single call (if 0, it will be converted to -1).
 download_blocksize: 1048576  # = 1024*1024
 # how many parallel threads to start when downloading (one thread per download)
 # If 0 or negative, is automatically set according to the machine CPU
 # (https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor)
 max_thread_workers: 0
 # max time to wait (in seconds) for a single request while downloading stations+channel metadata (`urllib2.urlopen` timeout argument)
 s_timeout: 120
 # If the flag to download station inventories is on (true), max time to wait (in seconds) for a single request
 # while downloading an inventory in xml format (`urllib2.urlopen` timeout argument)
 i_timeout: 60
 # max time to wait (in seconds) for a single request while downloading waveform data (`urllib2.urlopen` timeout argument)
 w_timeout: 60
 # the buffer size used when writing items (stations, segments, events, ...) to database.
 # Increasing this number usually sppeds speed up db IO operations, but increases the potentially unsaved
 # rows in case of errors, as if a row item in the buffer cannot be inserted or updated
 # (e.g., integrity errors), all subsequent buffer items will be also discarded
 db_buf_size: 20


