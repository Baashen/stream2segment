# -*- coding: utf-8 -*-
# from __future__ import print_function

"""module holding the basic functionalities of the download routine
   :Platform:
       Mac OSX, Linux
   :Copyright:
       Deutsches GFZ Potsdam <XXXXXXX@gfz-potsdam.de>
   :License:
       To be decided!
"""
import logging
from collections import defaultdict
from datetime import timedelta
from urlparse import urlparse
from itertools import izip, imap, repeat
import concurrent.futures
import numpy as np
import pandas as pd
from sqlalchemy.exc import SQLAlchemyError
# from obspy.taup.tau import TauPyModel
# from obspy.taup.helper_classes import TauModelError, SlownessModelError
from stream2segment.utils import get_progressbar, msgs
from stream2segment.utils.url import urlread, read_async, URLException
from stream2segment.io.db.models import Class, Event, DataCenter, Segment, Channel, Station
from stream2segment.io.db.pd_sql_utils import commit, colnames, withdata, Adder, dfrowiter
from stream2segment.download.utils import empty, urljoin, query2dframe, normalize_fdsn_dframe,\
    get_search_radius, calculate_times, UrlStats, stats2str, get_inventory_url, save_inventory,\
    dfupdate


logger = logging.getLogger(__name__)

ADDBUFSIZE = 10


def get_events_df(session, eventws, **args):
    url = urljoin(eventws, format='text', **args)
    try:
        raw_data = urlread(url, decode='utf8')
        events_df = query2df(url, raw_data, "event")  # this raises in case of errors

        adder = Adder(session, Event.id, add_buf_size=ADDBUFSIZE)
        events_df = adder.add(events_df)

        if empty(events_df):
            raise Exception(msgs.format(msgs.DB_ITEM_DISCARDED_(len(events_df)), url))

        if adder.discarded:
            logger.warning(msgs.format(msgs.DB_ITEM_DISCARDED_(adder.discarded), url))

        if adder.discarded:
            logger.info(msgs.format(msgs.DB_NEWITEM_SAVED_(adder.new), url))

        # drop columns we are not interested in (this saves memory??)
        events_df.drop([Event.author.key, Event.catalog.key, Event.contributor.key,
                        Event.contributor_id.key,
                        Event.event_location_name.key, Event.mag_author.key,
                        Event.mag_type.key], axis=1, inplace=True)
        return events_df
    except Exception as exc:
        if hasattr(exc, 'exc'):  # stream2segment URLException
            exc = exc.exc
        msg = msgs.format(exc, url)
        logger.error(msg)
        raise ValueError(msg)


def query2df(url, raw_data, dbmodel_key):
    """Returns a normalized and harmonized dataframe from raw_data. dbmodel_key can be 'event'
    'station' or 'channel'. Raises ValueError if the resulting dataframe is empty or if
    a ValueError is raised from sub-functions"""

    if not raw_data:
        # query2dframe below handles empty data, but we want meaningful log:
        raise ValueError(msgs.format(msgs.EMPTY_RESPONSE, url))

    try:
        dframe = query2dframe(raw_data)  # this raises ValueError, also if dframe is empty
    except ValueError as exc:
        raise ValueError(msgs.format(exc, url))

    # dframe surely not empty:
    try:
        oldlen, dframe = len(dframe), normalize_fdsn_dframe(dframe, dbmodel_key)
        # stations_df surely not empty:
        if oldlen > len(dframe):
            logger.warning(msgs.format(msgs.DF_ITEM_DISCARDED_((oldlen - len(dframe)), url)))
        return dframe
    except ValueError as exc:
        raise ValueError(msgs.format(exc, url))


def get_datacenters_df(session, **query_args):
    """Queries 'http://geofon.gfz-potsdam.de/eidaws/routing/1/query' for all datacenters
    available
    Rows already existing (comparing by datacenter station_query_url) are returned as well,
    but not added again
    :param query_args: any key value pair for the url. Note that 'service' and 'format' will
    be overiidden in the code with values of 'station' and 'format', repsecively
    """
    # do not return only new datacenters, return all of them
    query_args['service'] = 'station'
    query_args['format'] = 'post'
    url = urljoin('http://geofon.gfz-potsdam.de/eidaws/routing/1/query', **query_args)
    # add datacenters. Sql-alchemy way, cause we want the ids be autogenerated by the db
    sta2ids = {k.station_query_url: k for k in session.query(DataCenter)}
    try:
        dc_result = urlread(url, decode='utf8')
        # add to db the datacenters read. Two little hacks:
        # 1) parse dc_result string and assume any new line starting with http:// is a valid station
        # url url
        # 2) When adding the datacenter, the table column dataselect_query_url (when not provided,
        # as in this case) is assumed to be the same as station_query_url by replacing "/station"
        # with "/dataselect". See https://www.fdsn.org/webservices/FDSN-WS-Specifications-1.1.pdf
        datacenters = [DataCenter(station_query_url=dcen) for dcen in dc_result.split("\n")
                       if dcen[:7] == "http://"]
        dropped = 0
        new = 0
        for d in datacenters:
            if d.station_query_url not in sta2ids:
                try:
                    session.add(d)
                    session.commit()
                    new += 1
                    sta2ids[d.station_query_url] = d
                except SQLAlchemyError as exc:
                    session.rollback()
                    dropped += 1

        if dropped:
            logger.warning(msgs.format(msgs.DB_ITEM_DISCARDED_(dropped), url))
        if new:
            logger.info(msgs.DB_ITEM_DISCARDED_(new))

    except URLException as urlexc:
        msg = msgs.format(urlexc.exc, url)
        if not sta2ids:
            logger.error(msg)
            raise ValueError(msg)
        else:
            logger.warning(msg)
            logger.info(msgs.format("No datacenter downloaded, working with already "
                                    "saved datacenters (%d)" % len(datacenters)))
    dc_df = pd.DataFrame(data=[{DataCenter.id.key: x.id,
                                DataCenter.station_query_url.key: x.station_query_url,
                                DataCenter.dataselect_query_url.key: x.dataselect_query_url}
                               for x in sta2ids.values()])

    return dc_df


def get_fdsn_channels_df(session, events_df, datacenters_df, sradius_minmag, sradius_maxmag,
                         sradius_minradius, sradius_maxradius, station_timespan, channels,
                         min_sample_rate, max_thread_workers, timeout, blocksize,
                         notify_progress_func=lambda *a, **v: None):
    """Returns dict {event_id: stations_df} where stations_df is an already normalized and
    harmonized dataframe with the stations saved or already present, and the JOINT fields
    of Station and Channel. The id column values refers to Channel id's
    though"""
    stats = defaultdict(lambda: UrlStats())  # {d.station_query_url: UrlStats() for d in datacenters.itervalues()}
    ret = []

    def ondone(obj, result, exc, cancelled):  # pylint:disable=unused-argument
        """function executed when a given url has successfully downloaded data"""
        notify_progress_func(1)
        evt_id, dcen_id, url = obj[0], obj[1], obj[2]
        if cancelled:  # should never happen, however...
            msg = "Download cancelled"
            logger.warning(msgs.format(msg, url))
            stats[dcen_id][msg] += 1
        elif exc:
            logger.warning(msgs.format(exc, url))
            stats[dcen_id][exc] += 1
        else:
            df = get_stations_df(url, result, min_sample_rate)
            if empty(df):
                if result:
                    stats[dcen_id]['Malformed'] += 1
                else:
                    stats[dcen_id]['Empty'] += 1
            else:
                stats[dcen_id]['OK'] += 1
                df[Station.datacenter_id.key] = dcen_id
                df[Segment.event_id.key] = evt_id
                ret.append(df)

    iterable = evdcurl_iter(events_df, datacenters_df, sradius_minmag, sradius_maxmag,
                            sradius_minradius, sradius_maxradius, station_timespan, channels)

    read_async(iterable, ondone, urlkey=lambda obj: obj[-1], blocksize=blocksize,
               max_workers=max_thread_workers, decode='utf8', timeout=timeout)

    return pd.concat(ret, axis=0, ignore_index=True, copy=False), stats


def evdcurl_iter(events_df, datacenters_df, sradius_minmag, sradius_maxmag, sradius_minradius,
                 sradius_maxradius, station_timespan, channels):
    """returns an iterable of tuple (event, datacenter, station_query_url) where the last element
    is build with sradius_* arguments and station_timespan and channels"""
    # calculate search radia:
    max_radia = get_search_radius(events_df[Event.magnitude.key].values, sradius_minmag,
                                  sradius_maxmag, sradius_minradius, sradius_maxradius)

    # dfrowiter yields dicts with pythojn objects
    python_dcs = list(dfrowiter(datacenters_df, [DataCenter.id.key,
                                                 DataCenter.station_query_url.key]))

    for max_radius, evt_dict in izip(max_radia, dfrowiter(events_df)):
        evt_time = evt_dict[Event.time.key]
        evt_lat = evt_dict[Event.latitude.key]
        evt_lon = evt_dict[Event.longitude.key]
        start = evt_time - timedelta(hours=station_timespan[0])
        end = evt_time + timedelta(hours=station_timespan[1])
        evt_id = evt_dict[Event.id.key]
        for dc_dict in python_dcs:
            dcen_station_query_url = dc_dict[DataCenter.station_query_url.key]
            url = urljoin(dcen_station_query_url,
                          latitude="%3.3f" % evt_lat,
                          longitude="%3.3f" % evt_lon,
                          maxradius=max_radius,
                          start=start.isoformat(), end=end.isoformat(),
                          channel=','.join(channels), format='text', level='channel')
            yield (evt_id, dc_dict[DataCenter.id.key], url)


def get_stations_df(url, raw_data, min_sample_rate):
    """FIXME: write doc! """
    try:
        stations_df = query2df(url, raw_data, "channel")
    except ValueError as exc:
        logger.warning(msgs.format(exc, url))
        return empty()

    if min_sample_rate > 0:
        srate_col = Channel.sample_rate.key
        oldlen, stations_df = len(stations_df), \
            stations_df[stations_df[srate_col] >= min_sample_rate]
        reason = "sample rate < %s Hz" % str(min_sample_rate)
        if oldlen > len(stations_df):
            logger.warning("%s: %s", reason,
                           msgs.format(msgs.DB_ITEM_DISCARDED_(oldlen-len(stations_df)), url))

        if empty(stations_df):
            return stations_df
        # http://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas
        stations_df.is_copy = False

    return stations_df  # might be empty


def save_stations_and_channels(stations_df, session):
    """
        stations_df is already harmonized. If saved, it is appended a column
        `Channel.station_id.key` with nonNull values
    """
    STA_ID = Station.id.key
    CHA_ID = Channel.id.key
    STA_NET = Station.network.key
    STA_STA = Station.station.key
    CHA_LOC = Channel.location.key
    CHA_CHA = Channel.channel.key
    CHA_STAID = Channel.station_id.key
    SEG_CHAID = Segment.channel_id.key
    STA_DCID = Station.datacenter_id.key
    SEG_DCID = Segment.datacenter_id.key
    STA_DROP_LABELS = [Station.elevation.key, Station.start_time.key, Station.end_time.key]
    CHA_DROP_LABELS = [Channel.azimuth.key, Channel.depth.key, Channel.dip.key,
                       Channel.sample_rate.key, Channel.scale.key, Channel.scale_freq.key,
                       Channel.scale_units.key, Channel.sensor_description.key]

    stations_adder = Adder(session, Station.id, ADDBUFSIZE)
    channels_adder = Adder(session, Channel.id, ADDBUFSIZE)

    stations_df[STA_ID] = stations_df[STA_NET].map(str) + "." + stations_df[STA_STA].map(str)
    # attempt to write only unique stations. We cannot trust the db as we use a buffer and
    # an sqlalchemy.integrityerror in the first item discards the following items (which might
    # be good). This assures if we have errors we do not have false negatives:
    unique_sta_df = stations_df.drop_duplicates(subset=[STA_ID], inplace=False)
    unique_sta_df = stations_adder.add(unique_sta_df)

    # filter only saved or existing stations
    stations_df = empty() if empty(unique_sta_df) else \
        stations_df[stations_df[STA_ID].isin(unique_sta_df[STA_ID])]

    if not empty(stations_df):
        # remove columns we do not use anymore (saves memory?):
        # (note: Station.site_name and Station.inventory_xml are custom columns, they are not in
        # the fdsn response. So do not remove them)
        stations_df.drop(STA_DROP_LABELS, axis=1, inplace=True)

        channels_df = stations_df  # rename just for making clear what we are handling from here on
        channels_df.rename(columns={STA_ID: CHA_STAID}, inplace=True)

        channels_df[CHA_ID] = channels_df[CHA_STAID].map(str) + "." + \
            stations_df[CHA_LOC].map(str) + "." + stations_df[CHA_CHA].map(str)

        channels_df = channels_adder.add(channels_df)
        # remove columns we do not use anymore (saves memory?):
        channels_df.drop(CHA_DROP_LABELS, axis=1, inplace=True)
        channels_df.reset_index(drop=True, inplace=True)  # to be safe
        # rename id to channel_id (for use in segments):
        channels_df.rename(columns={CHA_ID: SEG_CHAID, STA_DCID: SEG_DCID}, inplace=True)
    else:
        channels_df = empty()

    if stations_adder.discarded:
        logger.warning("stations: %s", msgs.DB_ITEM_DISCARDED_(stations_adder.discarded))
    if channels_adder.discarded:
        logger.warning("channels: %s", msgs.DB_ITEM_DISCARDED_(channels_adder.discarded))
    if stations_adder.new:
        logger.info("stations: %s", msgs.DB_NEWITEM_SAVED_(stations_adder.new))
    if channels_adder.new:
        logger.info("channels: %s", msgs.DB_NEWITEM_SAVED_(channels_adder.new))

    return channels_df


def save_inventories(session, stations, max_thread_workers, timeout,
                     download_blocksize, notify_progress_func=lambda *a, **v: None):
    def ondone(obj, result, exc, cancelled):
        notify_progress_func(1)
        sta, url = obj
        if exc:
            logger.warning(msgs.format(exc, url))
        else:
            if not result:
                logger.warning(msgs.format(msgs.EMPTY_RESPONSE, url))
                return
            try:
                save_inventory(result, sta)
            except (TypeError, SQLAlchemyError) as _:
                session.rollback()
                logger.warning("station xml %s: %s", str(sta.id),
                               msgs.format(msgs.format(msgs.DB_ITEM_DISCARDED_(1), url)))

    iterable = izip(stations, (get_inventory_url(sta) for sta in stations))
    read_async(iterable, ondone, urlkey=lambda obj: obj[1],
               max_workers=max_thread_workers,
               blocksize=download_blocksize, timeout=timeout)


def set_saved_dist_and_times(session, segments_df):
    # define col labels (strings):
    SEG_EVID = Segment.event_id.key
    STA_LAT = Station.latitude.key
    STA_LON = Station.longitude.key
    SEG_EVDIST = Segment.event_distance_deg.key
    SEG_ATIME = Segment.arrival_time.key

    flt = (Station.latitude.in_(pd.unique(segments_df[STA_LAT]))) & \
        (Station.longitude.in_(pd.unique(segments_df[STA_LON]))) & \
        (Segment.event_id.in_(pd.unique(segments_df[SEG_EVID])))

    data = session.query(Segment.event_distance_deg, Segment.arrival_time,
                         Station.latitude, Station.longitude,
                         Event.id).join(Segment.station, Segment.event).filter(flt).distinct().all()
    df_repl = pd.DataFrame(columns=[SEG_EVDIST, SEG_ATIME, STA_LAT, STA_LON, SEG_EVID],
                           data=data)
    segments_df[SEG_EVDIST] = None
    segments_df[SEG_ATIME] = pd.NaT  # necessary to coerce values to date later

    return dfupdate(segments_df, df_repl, [STA_LAT, STA_LON, SEG_EVID], [SEG_EVDIST, SEG_ATIME])


def get_dists_and_times(events_df, segments_df, wtimespan, traveltime_phases,
                        taup_model,
                        notify_progress_func=lambda *a, **v: None):
    """channels_df must have benn built as, e.g.:
    unique_cha_df = channels_df.drop_duplicates(subset=[Segment.event_id.key,
                                                        Station.latitude.key,
                                                        Station.longitude.key], inplace=False)"""
    # define col labels (strings):
    SEG_EVID = Segment.event_id.key
    STA_LAT = Station.latitude.key
    STA_LON = Station.longitude.key
    SEG_EVDIST = Segment.event_distance_deg.key
    SEG_ATIME = Segment.arrival_time.key
    SEG_STIME = Segment.start_time.key
    SEG_ETIME = Segment.end_time.key
    EVT_ID = Event.id.key
    EVT_LAT = Event.latitude.key
    EVT_LON = Event.longitude.key
    EVT_DEPTH = Event.depth_km.key
    EVT_TIME = Event.time.key

    unique_channels_df = segments_df[pd.isnull(segments_df[SEG_ATIME])]
    unique_channels_df = unique_channels_df.drop_duplicates(subset=[SEG_EVID,
                                                                    STA_LAT,
                                                                    STA_LON],
                                                            inplace=False)
    # advance the progress bar of a given factor, taking into account that we shrink the dataframe
    # (unique_channels_df). We might overflow the progress bar but who cares
    total, done, factor = len(segments_df), 0, np.true_divide(len(segments_df),
                                                              len(unique_channels_df))
    evtdict_cache = {}
    patime_data = {SEG_EVDIST: [], SEG_ATIME: [], STA_LAT: [], STA_LON: [], SEG_EVID: []}
    # We can use a with statement to ensure threads are cleaned up promptly
    with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:

        future_to_evtid = {}
        for stadict in dfrowiter(unique_channels_df, [SEG_EVID, STA_LAT, STA_LON]):
            evtid = stadict[SEG_EVID]
            sta_lat = stadict[STA_LAT]
            sta_lon = stadict[STA_LON]
            evtdict = evtdict_cache.get(evtid, None)
            if evtdict is None:
                evtdict = events_df[events_df[EVT_ID] == evtid].iloc[0].to_dict()
                evtdict_cache[evtid] = evtdict
            future_to_evtid[executor.submit(calculate_times,
                                            sta_lat, sta_lon,
                                            evtdict[EVT_LAT],
                                            evtdict[EVT_LON],
                                            evtdict[EVT_DEPTH],
                                            evtdict[EVT_TIME],
                                            traveltime_phases,
                                            taup_model)] = evtid, sta_lat, sta_lon

        for future in concurrent.futures.as_completed(future_to_evtid):
            incr = total-done if done + factor >= total else factor
            notify_progress_func(incr)
            done += incr
            evtdist, atime = None, None
            try:
                evt_id, sta_lat, sta_lon = future_to_evtid[future]
                evtdist, atime = future.result()
                # set arrival time only if non-null
                patime_data[SEG_EVDIST].append(evtdist)
                patime_data[SEG_ATIME].append(atime)
                patime_data[STA_LAT].append(sta_lat)
                patime_data[STA_LON].append(sta_lon)
                patime_data[SEG_EVID].append(evt_id)
            except Exception as exc:
                # evt_id = atime = None
                logger.warning("Error calculating arrival time: '%s'", str(exc))

    # assign data to segments:
    df_repl = pd.DataFrame(data=patime_data)
    segments_df = dfupdate(segments_df, df_repl, [SEG_EVID, STA_LAT, STA_LON],
                           [SEG_EVDIST, SEG_ATIME])

    # drop errors in arrival time:
    oldlen = len(segments_df)
    segments_df.dropna(subset=[SEG_ATIME, SEG_EVDIST], inplace=True)
    if oldlen > len(segments_df):
        logger.info("%d of %d segments discarded (error while calculating arrival time)",
                    oldlen-len(segments_df), len(segments_df))
    # set start time and end time:
    td0, td1 = timedelta(minutes=wtimespan[0]), timedelta(minutes=wtimespan[1])
    segments_df[SEG_STIME] = (segments_df[SEG_ATIME] - td0).dt.round('s')
    segments_df[SEG_ETIME] = (segments_df[SEG_ATIME] + td1).dt.round('s')
    return segments_df


def prepare_for_wdownload(session, datacenters_df, segments_df, retry_empty_segments):
    """returns the wave queries and sets them to null if segments are already downloaded.
    Returns also the ids of each segment which indicates if the segment already exists"""
    # init col labels:
    SEG_STIME = Segment.start_time.key
    SEG_ETIME = Segment.end_time.key
    SEG_CHID = Segment.channel_id.key
    SEG_ID = Segment.id.key
    SEG_WITHDATA = "__withdata"
    SEG_DCID = Segment.datacenter_id.key
    DC_ID = DataCenter.id.key
    DC_DSURL = DataCenter.dataselect_query_url.key
    STA_NET = Station.network.key
    STA_STA = Station.station.key
    CHA_LOC = Channel.location.key
    CHA_CHA = Channel.channel.key

    # segments_df[_SEGMENTS_DATAURL_COLNAME] = wqueries
    segments_df[Segment.id.key] = None
    segments_df[Segment.data.key] = b''

    # pd.unique returns needs conversion
    unique_startt = pd.unique(segments_df[SEG_STIME]).astype("M8[us]").astype(object)
    unique_endt = pd.unique(segments_df[SEG_ETIME]).astype("M8[us]").astype(object)

    flt = (Segment.channel_id.in_(pd.unique(segments_df[SEG_CHID]))) & \
        (Segment.start_time.in_(unique_startt)) & \
        (Segment.end_time.in_(unique_endt))

    data = session.query(Segment.channel_id, Segment.start_time, Segment.end_time,
                         Segment.id, withdata(Segment.data)).filter(flt).all()

    df_repl = pd.DataFrame(columns=[SEG_CHID, SEG_STIME, SEG_ETIME, SEG_ID, SEG_WITHDATA],
                           data=data)

    segments_df[SEG_WITHDATA] = False
    segments_df = dfupdate(segments_df, df_repl, [SEG_CHID, SEG_STIME, SEG_ETIME],
                           [SEG_ID, SEG_WITHDATA])

    if not retry_empty_segments:
        segments_df.loc[~pd.isnull(segments_df[SEG_ID]), SEG_WITHDATA] = True

    oldlen = len(segments_df)
    segments_df = segments_df[segments_df[SEG_WITHDATA] != True]
    if oldlen != len(segments_df):
        logger.info("%d segments discarded (already downloaded with 'retry' flag=%s)",
                    oldlen-len(segments_df), str(retry_empty_segments))
    segments_df.is_copy = False

    # now set the data column. We do a loop via izip cause is at the end more readable than
    # pd.apply. The latter does Series conversion which MIGHT be more time consuming. And at the
    # end the code is not so readable.

    # First define a mapping datacenter to query:
    # add a column to datacenters to match 'datacenter_id' on segments:
    datacenters_df[SEG_DCID] = datacenters_df[DC_ID]
    segments_df[DC_DSURL] = ''
    # set a new column on segments_df to match datacenters urls:
    segments_df = dfupdate(segments_df, datacenters_df, [SEG_DCID], [DC_DSURL])
    # iteration over dframe columns is faster than
    # DataFrame.itertuples (as it does not have to convert each row to tuple)
    # and is more readable. Note: we zip using dataframe[columname] iterables. Using
    # dataframe[columname].values (underlying numpy array) is even faster, BUT IT DOES NOT RETURN
    # pd.TimeStamp objects for date-time-like columns (returns np.datetim64 instead).
    # As the former subclasses python datetime (so it's sqlalchemy compatible) and the second not,
    # we do not go for this solution, nor
    # for calling pd.TimeStamp(numpy_datetime) inside the loop (which is less readable):
    queries = []
    for dcen_url, net, sta, loc, cha, start_time, end_time \
        in izip(segments_df[DC_DSURL], segments_df[STA_NET], segments_df[STA_STA],
                segments_df[CHA_LOC], segments_df[CHA_CHA], segments_df[SEG_STIME],
                segments_df[SEG_ETIME]):
        queries.append(urljoin(dcen_url, network=net, station=sta, location=loc,
                               channel=cha, start=start_time.isoformat(), end=end_time.isoformat()))

    segments_df.is_copy = False
    # set index as the queries to download. Search by index is faster. Note that the column
    # _SEGMENTS_DATAURL_COLNAME is removed by default in set_index, but the index name keeps
    # the column name. That's harmless, but we remove it in `segments_df.index.name = None` below
    segments_df.index = queries
    segments_df.index.name = None
    # remove duplicated queries: it should never happen. However, in case,
    # the segments downloaded will not be correctly placed at the right index.
    # Thus,in case, remove all duplicates
    # http://stackoverflow.com/questions/13035764/remove-rows-with-duplicate-indices-pandas-dataframe-and-timeseries
    lendf = len(segments_df)
    segments_df = segments_df[~segments_df.index.duplicated(keep='first')]
    if lendf != len(segments_df):
        logger.warning(msgs.format(("Cannot handle %d duplicates in segments urls: discarded")),
                       lendf-len(segments_df))
    # drop unnecessary columns:
    segments_df.drop([DC_DSURL, STA_NET, STA_STA, CHA_LOC, CHA_CHA, SEG_WITHDATA], axis=1,
                     inplace=True)
    return segments_df


def download_segments(session, segments_df, run_id, max_error_count, max_thread_workers,
                      timeout, download_blocksize, sync_session_on_update='evaluate',
                      notify_progress_func=lambda *a, **v: None):
    # define column(s) string labels:
    SEG_DATA = Segment.data.key
    SEG_DCID = Segment.datacenter_id.key
    SEG_ID = Segment.id.key
    SEG_RUNID = Segment.run_id.key

    stats = defaultdict(lambda: UrlStats())
    if empty(segments_df):
        return stats
    errors = defaultdict(int)

    def ondone(obj, result, exc, cancelled):
        """function executed when a given url has succesfully downloaded `data`"""
        notify_progress_func(1)
        url, dcen_id = obj[0], obj[1]
        if cancelled:
            key_skipped = ("Discarded: Cancelled remaining downloads from the given data-center "
                           "after %d previous errors") % max_error_count
            stats[dcen_id][key_skipped] += 1
        elif exc:
            logger.warning("%s %s", str(exc), msgs.format(msgs.DB_ITEM_DISCARDED_(1), url))
            stats[dcen_id][exc] += 1
            errors[dcen_id] += 1
            if max_error_count > 0 and errors[dcen_id] == max_error_count:
                # skip remaining datacenters
                return lambda obj: obj[1] == dcen_id
        else:
            # avoid pandas SettingWithCopyWarning:
            segments_df.loc[url, SEG_DATA] = result  # might be empty: b''

    # now download Data:
    # we use the index as urls cause it's much faster when locating a dframe row compared to
    # df[df[df_urls_colname] == some_url]). We zip it with the datacenters for faster search
    # REMEMBER: iterating over series values is FASTER BUT USES underlying numpy types, and for
    # datetime's is a problem cause pandas sublasses datetime, numpy not
    read_async(izip(segments_df.index.values,
                    segments_df[SEG_DCID].values),
               urlkey=lambda obj: obj[0],
               ondone=ondone, max_workers=max_thread_workers,
               timeout=timeout, blocksize=download_blocksize)

    # messages:
    RETRY_NODATA_MSG = "Discarded: retry failed (zero bytes received or query error)"
    RETRY_WITHDATA_MSG = "Saved: retry successful (waveform data received)"
    NEW_NODATA_MSG = "Saved, no waveform data: zero bytes received or query error"
    NEW_WITHDATA_MSG = "Saved, with waveform data"
    DB_FAILED_MSG = "Discarded, error while saving data: local db error"

    not_added = 0
    if not empty(segments_df):
        notnull = segments_df[SEG_ID].notnull()
        segments_to_add_df = segments_df[~notnull]
        adder = Adder(session, Segment.id, ADDBUFSIZE)
        # hack to avoid checking if exists:
        adder.existing_keys = set()
        ret = adder.add(segments_to_add_df)
        oks = np.count_nonzero(np.asarray(ret))
        not_added = len(segments_to_add_df) - oks
        if not_added:
            logger.warning("%d of %d segments not saved (internal db error)", not_added,
                           len(segments_to_add_df))
            # also add as info (which by default prints to screen, if run from terminal):
            logger.info("%d of %d segments not saved (internal db error)", not_added,
                        len(segments_to_add_df))

        segments_to_update_df = segments_df[notnull]
        # http://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas
#        segments_df.is_copy = False  # FIXME: still need it?
        # segments_df[Segment.run_id.key] = run_id  # we will do it later
        last = len(segments_to_update_df) - 1
        buf = []
        for i, rowdict in enumerate(dfrowiter(segments_to_update_df)):
            model_instance = Segment(**rowdict)
            # if no data is found (empty or error), skip it:
            if not model_instance.data:
                stats[model_instance.datacenter_id][(RETRY_NODATA_MSG)] += 1
                continue
            # now we will either update or add the new segment. Set the run_id first:
            # already downloaded, but this time data was found
            # (if we attempted an already downloaded, it means segment.data was empty or None):
            # note that we do not update run_id
            stats[model_instance.datacenter_id][RETRY_WITHDATA_MSG] += 1
            session.query(Segment).filter(Segment.id == model_instance.id).\
                update({SEG_DATA: model_instance.data, SEG_RUNID: run_id},
                       synchronize_session=sync_session_on_update)
            buf.append(model_instance)
            if (i == last and buf) or len(buf) == ADDBUFSIZE:
                if commit(session):
                    msg = NEW_WITHDATA_MSG if model_instance.data else NEW_NODATA_MSG
                    for m in buf:
                        stats[m.datacenter_id][msg] += 1
                else:
                    # we rolled back
                    for m in buf:
                        stats[m.datacenter_id][DB_FAILED_MSG] += 1
                buf = []

    return stats


def add_classes(session, class_labels):
    if class_labels:
        cdf = pd.DataFrame(data=[{Class.label.key: k, Class.description.key: v}
                           for k, v in class_labels.iteritems()])
        adder = Adder(session, Class.label, ADDBUFSIZE)
        adder.add(cdf)


def main(session, run_id, start, end, eventws, eventws_query_args, stimespan,
         sradius_minmag, sradius_maxmag, sradius_minradius, sradius_maxradius,
         channels, min_sample_rate, download_s_inventory, traveltime_phases,
         wtimespan,
         retry_empty_segments,
         advanced_settings, class_labels=None, isterminal=False):
    """
        Downloads waveforms related to events to a specific path
        :param eventws: Event WS to use in queries. E.g. 'http://seismicportal.eu/fdsnws/event/1/'
        :type eventws: string
        :param eventws_query_args: a dict of fdsn arguments to be passed to the eventws query. E.g.
        {'minmag' : 3.0}
        :param search_radius_args: The arguments required to get the search radius R whereby all
            stations within R will be queried from a given event location E_lat, E_lon
        :type search_radius_args: list or iterable of numeric values:
            (min_magnitude, max_magnitude, min_distance, max_distance)
        :param datacenters_dict: a dict of data centers as a dictionary of the form
            {name1: url1, ..., nameN: urlN} where url1, url2,... are strings
        :type datacenters_dict dict of key: string entries
        :param channels: iterable (e.g. list) of channels (as strings), e.g.
            ['HH?', 'SH?', 'BH?', 'HN?', 'SN?', 'BN?']
        :type channels: iterable of strings
        :param start: Limit to events on or after the specified start time
            E.g. (date.today() - timedelta(days=1))
        :type start: datetime
        :param end: Limit to events on or before the specified end time
            E.g. date.today().isoformat()
        :type end: datetime
        :param ptimespan: the minutes before and after P wave arrival for the waveform query time
            span
        :type ptimespan: iterable of two float
        :param min_sample_rate: the minimum sample rate required to download data
        channels with a field 'SampleRate' lower than this value (in Hz) will be discarded and
        relative data not downloaded
        :type min_sample_rate: float
        :param session: sql alchemy session object
        :type outpath: string
    """
    # set blocksize if zero:
    if advanced_settings['download_blocksize'] <= 0:
        advanced_settings['download_blocksize'] = -1
    if advanced_settings['max_thread_workers'] <= 0:
        advanced_settings['max_thread_workers'] = None

    progressbar = get_progressbar(isterminal)

    stepiter = imap(lambda i, m: "%d of %d" % (i+1, m), xrange(8 if download_s_inventory else 7),
                    repeat(8 if download_s_inventory else 7))

    # write the class labels:
    add_classes(session, class_labels)

    startiso = start.isoformat()
    endiso = end.isoformat()

    # events and datacenters should raise exceptions so that we can print the message in case.
    # Delegating the log only does not help a user when the program stops so quickly
    try:
        # Get events, store them in the db, returns the event instances (db rows) correctly added:
        logger.info("")
        logger.info("STEP %s: Querying events", next(stepiter))
        events_df = get_events_df(session, eventws, start=startiso, end=endiso,
                                  **eventws_query_args)
        # Get datacenters, store them in the db, returns the dc instances (db rows) correctly added:
        logger.info("")
        logger.info("STEP %s: Querying datacenters", next(stepiter))
        datacenters_df = get_datacenters_df(session, start=startiso, end=endiso)
    except Exception as exc:
        if isterminal:
            print str(exc)
        return 1

    logger.info("")
    logger.info(("STEP %s: Querying stations (level=channel, datacenter(s): %d) "
                 "nearby %d event(s) found"), next(stepiter), len(datacenters_df), len(events_df))

    with progressbar(length=len(events_df)*len(datacenters_df)) as bar:
        stations_df, s_stats = get_fdsn_channels_df(session, events_df, datacenters_df,
                                                    sradius_minmag, sradius_maxmag,
                                                    sradius_minradius, sradius_maxradius,
                                                    stimespan, channels, min_sample_rate,
                                                    advanced_settings['max_thread_workers'],
                                                    advanced_settings['s_timeout'],
                                                    advanced_settings['download_blocksize'],
                                                    bar.update)

    logger.info("")
    logger.info(("STEP %s: Saving stations and channels to db"), next(stepiter))
    channels_df = save_stations_and_channels(stations_df, session)

    if download_s_inventory:
        stations = session.query(Station).filter(~withdata(Station.inventory_xml)).all()
        logger.info("")
        logger.info(("STEP %s: Downloading %d stations inventories"), next(stepiter), len(stations))
        with progressbar(length=len(stations)) as bar:
            save_inventories(session, stations,
                             advanced_settings['max_thread_workers'],
                             advanced_settings['i_timeout'],
                             advanced_settings['download_blocksize'], bar.update)

    logger.info("")
    logger.info(("STEP %s: Calculating P-arrival times "
                 "and time ranges"), next(stepiter))
    # rename dataframe to make clear that now we have segments:
    segments_df = set_saved_dist_and_times(session, channels_df)
    segments_df.is_copy = False  # avoid pandas setting with copy warnings
    with progressbar(length=len(segments_df)) as bar:
        segments_df = get_dists_and_times(events_df, segments_df, wtimespan, traveltime_phases,
                                          'ak135', bar.update)

    logger.info("")
    logger.info(("STEP %s: Checking already downloaded segments"), next(stepiter))
    segments_df = prepare_for_wdownload(session, datacenters_df, segments_df, retry_empty_segments)

    segments_count = len(segments_df)
    logger.info("")
    logger.info("STEP %s: Querying Datacenter WS for %d segments", next(stepiter), segments_count)

    with progressbar(length=segments_count) as bar:
        d_stats = download_segments(session, segments_df, run_id,
                                    advanced_settings['w_maxerr_per_dc'],
                                    advanced_settings['max_thread_workers'],
                                    advanced_settings['w_timeout'],
                                    advanced_settings['download_blocksize'],
                                    False,  # do not sync on update
                                    bar.update)

    logger.info("")

    # define functions to represent stats:
    def rfunc(row):
        """function for modifying each row display"""
        url_ = datacenters_df[datacenters_df[DataCenter.id.key] ==
                              row][DataCenter.station_query_url.key].iloc[0]
        return urlparse(url_).netloc

    def cfunc(col):
        """function for modifying each col display"""
        return col if col.find(":") < 0 else col[:col.find(":")]

    logger.info("Summary Station WS query info:")
    logger.info(stats2str(s_stats, fillna=0, transpose=True, lambdarow=rfunc, lambdacol=cfunc,
                          sort='col'))
    logger.info("")
    logger.info("Summary Datacenter WS info :")
    logger.info(stats2str(d_stats, fillna=0, transpose=True, lambdarow=rfunc, lambdacol=cfunc,
                          sort='col'))
    logger.info("")

    return 0
